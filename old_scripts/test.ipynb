{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\pyarrow\\pandas_compat.py:571: UserWarning: pyarrow requires pandas 1.0.0 or above, pandas 0.25.1 is installed. Therefore, pandas-specific integration is not used.\n",
      "  columns)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "pyarrow requires pandas 1.0.0 or above, pandas 0.25.1 is installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11932\\206604360.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Crea un dataset Hugging Face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[1;34m(cls, df, features, info, split, preserve_index)\u001b[0m\n\u001b[0;32m    848\u001b[0m         table = InMemoryTable.from_pandas(\n\u001b[0;32m    849\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 850\u001b[1;33m             \u001b[0mpreserve_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreserve_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m         )\n\u001b[0;32m    852\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\datasets\\table.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \"\"\"\n\u001b[1;32m--> 761\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    569\u001b[0m      \u001b[0mcolumns_to_convert\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m      \u001b[0mconvert_fields\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_columns_to_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m                                                columns)\n\u001b[0m\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;31m# NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36m_get_columns_to_convert\u001b[1;34m(df, schema, preserve_index, columns)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_level\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_levels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_index_level_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         if (isinstance(index_level, _pandas_api.pd.RangeIndex) and\n\u001b[0m\u001b[0;32m    386\u001b[0m                 preserve_index is None):\n\u001b[0;32m    387\u001b[0m             \u001b[0mdescr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_range_index_descriptor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\pyarrow\\pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim.pd.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\pyarrow\\pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim._check_import\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luca-\\OneDrive\\Desktop\\University\\#MAGISTRALE\\2 ANNO\\1 SEMESTRE\\DEEP LEARNING\\project\\informer_project_dl\\.venv\\lib\\site-packages\\pyarrow\\pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim._import_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: pyarrow requires pandas 1.0.0 or above, pandas 0.25.1 is installed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Carica il file CSV\n",
    "df = pd.read_csv('.\\data\\predictive_maintenance.csv')\n",
    "\n",
    "# Rimuovi le colonne 'Target' e 'Failure Type'\n",
    "df = df.drop(columns=['Target', 'Failure Type'])\n",
    "\n",
    "# Crea un dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InformerConfig\n",
    "\n",
    "config = InformerConfig(\n",
    "    prediction_length=24,  # Numero di step futuri da prevedere\n",
    "    context_length=48,     # Numero di step passati da considerare\n",
    "    num_time_features=2,   # Numero di caratteristiche temporali\n",
    "    num_static_real_features=6,  # Numero di caratteristiche statiche reali\n",
    "    lags_sequence=[1, 2, 3, 4, 5, 6, 7]  # Sequenza di lag\n",
    ")\n",
    "\n",
    "from transformers import InformerForPrediction\n",
    "\n",
    "model = InformerForPrediction(config)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definisci una funzione per creare sequenze di dati\n",
    "def create_sequences(dataset, context_length, prediction_length):\n",
    "    sequences = []\n",
    "    for i in range(len(dataset) - context_length - prediction_length):\n",
    "        past = dataset[i:i + context_length]\n",
    "        future = dataset[i + context_length:i + context_length + prediction_length]\n",
    "        sequences.append((past, future))\n",
    "    return sequences\n",
    "\n",
    "# Crea le sequenze\n",
    "sequences = create_sequences(dataset, config.context_length, config.prediction_length)\n",
    "\n",
    "# Crea un DataLoader\n",
    "dataloader = DataLoader(sequences, batch_size=32, shuffle=True)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definisci una funzione per creare sequenze di dati\n",
    "def create_sequences(dataset, context_length, prediction_length):\n",
    "    sequences = []\n",
    "    for i in range(len(dataset) - context_length - prediction_length):\n",
    "        past = dataset[i:i + context_length]\n",
    "        future = dataset[i + context_length:i + context_length + prediction_length]\n",
    "        sequences.append((past, future))\n",
    "    return sequences\n",
    "\n",
    "# Crea le sequenze\n",
    "sequences = create_sequences(dataset, config.context_length, config.prediction_length)\n",
    "\n",
    "# Crea un DataLoader\n",
    "dataloader = DataLoader(sequences, batch_size=32, shuffle=True)\n",
    "\n",
    "from transformers import InformerForPrediction\n",
    "import torch.optim as optim\n",
    "\n",
    "# Inizializza il modello\n",
    "model = InformerForPrediction(config)\n",
    "\n",
    "# Definisci l'ottimizzatore\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Definisci la funzione di perdita\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Ciclo di addestramento\n",
    "model.train()\n",
    "for epoch in range(10):  # Numero di epoche\n",
    "    total_loss = 0\n",
    "    for past, future in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Estrai le caratteristiche temporali e statiche\n",
    "        past_values = past[:, :, 2:].float()\n",
    "        past_time_features = past[:, :, :2].float()\n",
    "        future_values = future[:, :, 2:].float()\n",
    "        future_time_features = future[:, :, :2].float()\n",
    "        # Maschera di osservazione\n",
    "        past_observed_mask = torch.ones_like(past_values)\n",
    "        future_observed_mask = torch.ones_like(future_values)\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            future_time_features=future_time_features\n",
    "        )\n",
    "        # Calcola la perdita\n",
    "        loss = loss_fn(outputs.logits, future_values)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoca {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for past, future in dataloader:\n",
    "        # Estrai le caratteristiche temporali e statiche\n",
    "        past_values = past[:, :, 2:].float()\n",
    "        past_time_features = past[:, :, :2].float()\n",
    "        future_values = future[:, :, 2:].float()\n",
    "        future_time_features = future[:, :, :2].float()\n",
    "        # Maschera di osservazione\n",
    "        past_observed_mask = torch.ones_like(past_values)\n",
    "        future_observed_mask = torch.ones_like(future_values)\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            future_time_features=future_time_features\n",
    "        )\n",
    "        # Calcola la perdita o altre metriche di valutazione\n",
    "        loss = loss_fn(outputs.logits, future_values)\n",
    "        print(f'Loss di valutazione: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
